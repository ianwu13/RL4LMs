"""
Observe the sequence generated by the trained model.
"""

import argparse
import glob
import os
import json
import time
import logging
import random
import re
from itertools import chain
from string import punctuation
from random import randint
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl


from transformers import (
    AdamW,
    T5ForConditionalGeneration,
    T5Tokenizer,
    AutoTokenizer,
    get_linear_schedule_with_warmup
)


# params
model_dir = "/home/ICT2000/chawla/nego_rl/offline_rl/dummy_1/"

# load model.
model = T5ForConditionalGeneration.from_pretrained(model_dir)
tokenizer = T5Tokenizer.from_pretrained(model_dir)

#loop to use T5 iteratively.

def count_prev_as(lst):
    cnt = 0
    for item in lst:
        if item == 'a':
            cnt += 1
    return cnt

def get_new_rtg(rtgs, traj, out):
    """
    basically, use the model output, compute the new rtgs for the model, and return.
    """
    
    score = 0
    if out != 'done':
        score -= 2
    if out == 'a':
        prev_as = count_prev_as(traj)
        if prev_as < 5:
            score += 10
    
    new_rtg = rtgs[-1] - score
    return new_rtg

def compute_total_reward(traj):
    total_reward = 0
    for i in range(len(traj)):

        score = 0
        if traj[i] != 'done':
            score -= 2
        if traj[i] == 'a':
            prev_as = count_prev_as(traj[:i][:])
            if prev_as < 5:
                score += 10
        
        total_reward += score
    
    return total_reward

curr_out = None
max_iters = 30

curr_traj = []
curr_rtgs = [40]

for _ in range(max_iters):

    # prepare input
    inp = f"reward-to-go: {' '.join([str(arr) for arr in curr_rtgs])} history: {' '.join([str(arr) for arr in curr_traj])}"

    # tokenize the input
    input_ids = tokenizer(inp, return_tensors="pt").input_ids

    # generate model output
    output_encodings = model.generate(
        input_ids,
        num_beams=1,
        do_sample=False,
        max_length=5)

    # decode output
    out = tokenizer.decode(output_encodings[0])
    print(inp, out)

    curr_rtgs.append(get_new_rtg(curr_rtgs, curr_traj, out))
    
    curr_traj.append(out)

    if curr_traj[-1] == 'done':
        break
    
print("Generated trajectory: ")
print(curr_traj)
print(curr_rtgs)

#compute the actual total reward of the trajectory.
total_reward = compute_total_reward(curr_traj)
print(total_reward)